#!/usr/bin/env python3

import urllib.request as urlreq
from bs4 import BeautifulSoup
import sys, re, os, codecs

help_str = '''
This is the help section for {}
Make sure to use the whole url such as:

http://www.google.com

Beautiful Soup required to run
if using ubuntu use:

sudo apt install python-bs4

A text file located within ~/spider_soup_out/ will be written with your
results.

there will be more info here soon
'''.format(sys.argv[0])

def main(url, max_depth = 2, depth = 0, url_root = '', url_buff = [], visited = []):

    link_list = []
    link_list_preprocess = []
    link_list_ultimate = []
    link_list_final = []
    visit = []
    if depth <= max_depth and url not in visited:
        visited.append(url)

        # attempt to call the user given URL to look for links
        try:
            headers = { 'User-Agent' : "Mozilla/5.0" }
            req = urlreq.Request(url, None, headers)
            html = urlreq.urlopen(req).read()

        # Return user friendly error if web address is no good
        except ValueError:
            print('Invalid URL')
            print('Use full url such as: https://www.google.com')

        except KeyboardInterrupt:
            sys.exit()
        
        # Return generic error message if unexpected error occours
        except:
            print('Exception occurred! See exception info below')
            print('Trying to build link list\n')
            print(sys.exc_info())
            return [], visited

        # Parse html code into workable document
        html_soup = BeautifulSoup(html, 'html.parser')

        # Print html to screen (optional)
        # print(html_soup.prettify()) 

        # find all links in html doc
        for link in html_soup.find_all('a'):
            link_list.append(link.get('href'))

        # assemble links into http:// form
        # discard unusable links
        for link in link_list:
            try:
                if link[0] == '/' and len(link) > 1:
                    if (url_root + link) not in link_list_final:
                        link_list_final.append(url_root + link)

                elif link[0:4].upper() == 'HTTP':
                    if link not in link_list_final:
                        link_list_final.append(link)

                elif link[0] == '/' and len(link) == 1:
                    pass

                else:
                    if (url_root + '/' + link) not in link_list_final:
                        link_list_final.append(url_root + '/' + link)

            except TypeError:
                pass

            except KeyboardInterrupt:
                sys.exit()

            except:
                print('Exception occurred! See exception info below')
                print('Trying to build link list\n')
                print(sys.exc_info())
                return [], visited


        for link in link_list_final:
            if link not in visited:
                print('Depth: {} Link: {}'.format(depth, link))
                link_list_preprocess, visit = main(link, max_depth, depth + 1, url_root, link_list_final, visited)
            # WORK HERE NOW
            for itm in visit:
                if itm not in visited:
                    visited.append(itm)

            for itm in link_list_preprocess:
                if itm not in link_list_ultimate:
                    link_list_ultimate.append(itm)

            for itm in url_buff:
                if itm not in link_list_ultimate:
                    link_list_ultimate.append(itm)

        return link_list_ultimate, visited

    else:
        visited.append(url)
        return [url], visited

if __name__ == '__main__':
    # program INIT
    if len(sys.argv) < 2:
        # usage string if not enough arguments
        print('Usage: {} <parent url to scan>'.format(sys.argv[0]))
        print('Enter {} --help for more info'.format(sys.argv[0]))
        sys.exit()

    # Look for help string option
    elif '--help' in sys.argv:
        print(help_str)
        sys.exit()

    else:
        robo_avoid = []
        args = sys.argv[1:]
        if '-m' in args:
            max_depth = int(args[args.index('-m') + 1])
            args.pop(args.index('-m') + 1)
            args.pop(args.index('-m'))
        
        if '-r' not in args:
            # Attempt to call the user given URL to look for links
            try:
                headers = { 'User-Agent' : "Mozilla/5.0" }
                req = urlreq.Request(args[0] + '/robots.txt', None, headers)
                html = urlreq.urlopen(req)
                for line in html:
                    robo_rule = ''
                    robo_obj = re.match(r'[^\/]+(\/\S+)', line.decode('unicode_escape'))
                    try:
                        robo_rule = line[robo_obj.start(1):robo_obj.end(1)].decode('unicode_escape')
                        robo_avoid.append(args[0] + robo_rule)
                    except AttributeError:
                        # skip blank lines or lines with no rules to match
                        continue
                    except:
                        print('Exception occurred! See exception info below')
                        print('Trying to build robo avoid list\n')
                        print(sys.exc_info())
                print(robo_avoid)

            # Return user friendly error if web address is no good
            except ValueError:
                print('Invalid URL')
                print('Use full url such as: https://www.google.com')
                sys.exit()

            except KeyboardInterrupt:
                sys.exit()
            
            # Return generic error message if unexpected error occours
            except:
                print('Exception occurred! See exception info below')
                print('Trying to build link list\n')
                print(sys.exc_info())
                sys.exit()

            '''
            # Parse html code into workable document
            robot_txt = BeautifulSoup(html, 'html.parser')
            '''

            # this is parsing as one line i need it to pare as many

        else:
            args.pop(args.index('-r'))

        try:
            # extract root url ex. http://www.google.com -> google.com
            urlobj = re.match(r'(?:https?|ftp)://(?:www.)?([^/\r\n]+)(/[^\r\n]*)?', args[0])
            rooturl = args[0][urlobj.start(1):urlobj.end(1)]
            # Create filename ex. google.com -> google.com.txt
            filename = rooturl + '.txt'
        except AttributeError:
            print('Invalid URL')
            print('Use full url such as: https://www.google.com')
            sys.exit()

        except KeyboardInterrupt:
            sys.exit()

        except:
            print('Exception occurred! See exception info below')
            print('Trying to extract root URL\n')
            print(sys.exc_info())
            sys.exit()

        # Create full path to users home folder and output directory
        filepath = os.path.expanduser('~/spider_soup_out/')

        # Check to see if output folder exists and create if not 
        if not os.path.exists(filepath):
            os.makedirs(filepath)

        # Call main function to run recursively and spider the web page
        url_out, visited = main(args[0], url_root = args[0], visited = robo_avoid)
        url_out.sort()

        # Write harvested links to file
        with open(filepath + filename, 'a') as urltext: 
            for line in url_out:
                urltext.write(line + '\n')

